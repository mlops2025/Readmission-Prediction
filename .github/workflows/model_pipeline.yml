name: Readmission Prediction CI/CD Pipeline
 
on:
  push:
    # branches: folder_struct
  # schedule:
  #   - cron: '0 0 * * *'  # Runs every day at midnight
  workflow_dispatch:
 
env:
  REGION: us-east1
  REPOSITORY_NAME: Readmission-Preddiction
  GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  GCS_BUCKET_NAME: ${{ secrets.GCS_BUCKET_NAME }}
 
jobs:
  build_test_and_deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
 
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
 
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
 
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
 
      - name: Authenticate with GCP
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
 
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
 
      - name: Set GCP project ID
        run: gcloud config set project "${{ secrets.GCP_PROJECT_ID }}"
 
### Move Docker Build and Push Steps Earlier
      # - name: Build and Push Docker Image
      #   env:
      #     IMAGE_NAME: ${{ env.REGION }}-docker.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/${{ env.REPOSITORY_NAME }}/model-image
      #   run: |
      #     docker build -t ${IMAGE_NAME}:${{ github.sha }} .
      #     docker push ${IMAGE_NAME}:${{ github.sha }}
      #     docker tag ${IMAGE_NAME}:${{ github.sha }} ${IMAGE_NAME}:latest
      #     docker push ${IMAGE_NAME}:latest
 
### Continue with Airflow Setup and Model Deployment
      - name: Create directories for Airflow
        run: |
          mkdir -p ./dags ./logs ./plugins

      - name: Set _PIP_ADDITIONAL_REQUIREMENTS from requirements.txt
        id: set_pip_reqs
        run: |
          echo "_PIP_ADDITIONAL_REQUIREMENTS=$(cat requirements.txt | tr '\n' ' ')" >> $GITHUB_ENV

 
      - name: Initialize Airflow Database
        run: |
          docker compose up airflow-init
        env:
          _PIP_ADDITIONAL_REQUIREMENTS: ${{ env._PIP_ADDITIONAL_REQUIREMENTS }}
 
      - name: Start Airflow services
        run: |
          docker compose up -d
        env:
          _PIP_ADDITIONAL_REQUIREMENTS: ${{ env._PIP_ADDITIONAL_REQUIREMENTS }}

      - name: Wait for Airflow to initialize
        run: |
          echo "Waiting for 180s to allow all Airflow services to come up..."
          sleep 180

      - name: Check Airflow services status
        run: docker compose ps

      - name: Print Airflow worker logs
        run: docker compose logs airflow-worker
      
      - name: Print Airflow scheduler logs
        run: docker compose logs airflow-scheduler

      - name: Print Redis logs
        run: docker compose logs redis

      - name: Check Airflow Webserver Health
        run: |
          echo "Checking if Airflow Webserver is healthy..."
          docker compose exec airflow-webserver curl -f http://localhost:8080/health || {
            echo "‚ùå Webserver health check failed!";
            docker compose logs airflow-webserver;
            exit 1;
          }

      - name: Check Airflow Worker Health
        run: |
          echo "Checking if Airflow Worker can respond to ping..."
          docker compose exec airflow-worker celery -A airflow.executors.celery_executor.app inspect ping || {
            echo "‚ùå Celery worker did not respond to ping!";
            docker compose logs airflow-worker;
            exit 1;
          }

      - name: Tail Airflow Scheduler logs (initial check)
        run: |
          echo "---- airflow-scheduler logs (last 50 lines) ----"
          docker compose logs --tail=50 airflow-scheduler || true

      - name: Trigger DAG and wait for completion
        run: |
          if docker compose ps | grep -q "airflow-webserver.*Up"; then
            echo "‚úÖ Airflow webserver is up. Triggering DAG..."

            DAG_ID="DataPipeline"
            RUN_ID="manual_$(date +%Y%m%dT%H%M%S)"
            docker compose exec airflow-webserver airflow dags trigger "$DAG_ID" --run-id "$RUN_ID"

            echo "Waiting for DAG run to complete (RUN_ID: $RUN_ID)..."

            for i in {1..15}; do
              echo "‚åõ [Attempt $i] Checking DAG run status..."
              RUN_INFO=$(docker compose exec airflow-webserver airflow dags list-runs -d "$DAG_ID" -o json | grep "$RUN_ID")
              
              if [[ -z "$RUN_INFO" ]]; then
                echo "üîÑ DAG run not found yet. Retrying in 30s..."
                sleep 30
                continue
              fi

              STATUS=$(echo "$RUN_INFO" | grep -o '"state": *"[^"]*"' | awk -F'"' '{print $4}')
              echo "üìå Current DAG status: $STATUS"

              if [[ "$STATUS" == "success" ]]; then
                echo "‚úÖ DAG completed successfully."
                break
              elif [[ "$STATUS" == "failed" ]]; then
                echo "‚ùå DAG failed. Showing recent logs:"
                docker compose logs --tail=100 airflow-scheduler
                docker compose logs --tail=100 airflow-worker
                exit 1
              fi

              echo "‚è≥ DAG is still running. Checking again in 60 seconds..."
              sleep 60
            done
          else
            echo "‚ùå Airflow webserver is not running. Current status:"
            docker compose ps
            exit 1
          fi

 
### Model Upload and Deployment
      # - name: Check for model file
      #   run: |
      #     echo "Current directory: $(pwd)"
      #     echo "Contents of current directory:"
      #     ls -R
      #     echo "Searching for best_model.pkl:"
      #     find . -name best_model.pkl
 
      # - name: Save and Upload Model to GCS
      #   run: |
      #     MODEL_FILE=$(find . -name best_model.pkl)
      #     if [ -z "$MODEL_FILE" ]; then
      #       echo "Error: best_model.pkl not found"
      #       exit 1
      #     fi
      #     echo "Uploading $MODEL_FILE to GCS"
      #     gsutil cp $MODEL_FILE gs://${{ env.GCS_BUCKET_NAME }}/models/best_model_${{ github.sha }}.pkl
 
      # - name: Deploy Model to AI Platform
      #   run: |
      #     gcloud ai models upload \
      #       --region=${{ env.REGION }} \
      #       --display-name=bank-marketing-model-${{ github.sha }} \
      #       --artifact-uri=gs://${{ env.GCS_BUCKET_NAME }}/models/best_model_${{ github.sha }}.pkl
 
### Final Cleanup and Notifications
      - name: Clean up
        if: always()
        run: docker compose down --volumes --rmi all
 
      - name: Deployment success message
        if: success()
        run: echo "CI/CD pipeline completed successfully. Model uploaded and services started."
 
      - name: Deployment failed notification
        if: failure()
        run: echo "CI/CD pipeline failed. Check the logs for details."